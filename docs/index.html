---
layout: default
title: Mean Shift for Self-Supervised Learning 
---

<div style="height:25px;">
<p style="text-align:center;"><a href="https://www.csee.umbc.edu/~soroush">Soroush Abbasi Koohpayegani</a><sup>∗</sup>, <a href="https://www.linkedin.com/in/ajinkya-tejankar/">Ajinkya Tejankar</a><sup>∗</sup>, <a href="https://www.csee.umbc.edu/~hpirsiav/">Hamed Pirsiavash</a></p>
</div>
<div style="height:25px;">
<p style="text-align:center;">University of Maryland, Baltimore County</p>
</div>
<div style="height:30px;">
<p style="text-align:center; font-size:12px"><sup>∗</sup> denote equal contribution</p>
</div>

<div class="menu">
  <ul style="margin: 0px;">
      <li><a href='https://arxiv.org/abs/2010.14713'>[Paper]</a></li>
      <li><a href='.'>[Poster]</a></li>
      <li><a href='https://github.com/UMBCvision/MSF'>[Code]</a></li>
      <li><a href='/MSF/bib.txt'>[Bib]</a></li>
  </ul>
</div>

<div>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.gif" width="100%" alt style></p>

<h5 id="abstract"><b>Abstract</b></h5>
<p>Most recent self-supervised learning (SSL) algorithms learn features by contrasting between instances of images or by clustering the 
  images and then contrasting between the image clusters. We introduce a simple mean-shift algorithm that learns representations by grouping images
  together without contrasting between them or adopting much of prior on the structure of the clusters. We simply "shift" the embedding of each
  image to be close to the "mean" of its neighbors. Since in our setting, the closest neighbor is always another augmentation of the same image,
  our model will be identical to BYOL when using only one nearest neighbor instead of 5 as used in our experiments. Our model achieves 72.4%
  on ImageNet linear evaluation with ResNet50 at $200$ epochs outperforming BYOL.</p>

<h5 id="contributions"><b>Contributions</b></h5>
  <p> 
    We introduce a simple but effective mean-shift algorithm to group similar images together in the neighborhood of each image in an 
    online fashion. The idea is to simply find the nearest neighbors of a query image in the embedding space and pull the embedding of 
    query to be closer to the center of those neighbors. We believe this process will result in developing clusters of images in the
    embedding space without enforcing much constraints about their specific size, number, or shape. Note that in contrast to grouping (pulling)
    in our method, MoCo pushes the query to be far from any other data points particularly nearest neighbors by which the loss will be dominated.
    
    
  </p>

  
    <p> 
  For two random query images, we show how the nearest neighbors evolve at the learning time. Initially, NNs are not semantically 
    quite related, but are close in low-level features. The accuracy of 1-NN classifier in the initialization is 1.5% which is 15
    times larger than random chance (0.01%). This little signal is bootstrapped in our learning method and results in NNs of the 
    late epochs which are mostly semantically related to the query image.
  </p>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/fig1.jpg" width="100%" alt style></p>
    
  

  
  
    <h5 id="Method"><b>Method</b></h5>
<p>
      Similar to BYOL, we maintain two encoders ("target" and "online") using momentum update for the target encoder. 
    We augment an image twice and feed to both encoders. We add the target embedding to the memory bank and look for its 
    nearest neighbors in the memory bank. Obviously target embedding itself will be the first nearest neighbor. We want to 
    shift the query image towards the mean of its nearest neighbors, so we minimize the summation of those distances.
    Note that our method using only one nearest neighbor is identical to BYOL which pulls different augmentations together
    without grouping different instances of images. To our knowledge, our method is the first in grouping different instances 
    of images without contrasting between image instances or clusters.
    </p>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.gif" width="90%" alt style></p>

<h5 id="results"><b>Self-supervised Learning Results</b></h5>

    <p>
       We compare our model on the full ImageNet linear and nearest neighbor benchmarks using ResNet50.
      We find that given similar computational budget, our models are better than other state-of-the-art methods. 
      Our w/s variation works slightly better than the regular MSF. Note that methods with symmetric loss are not 
      directly comparable with the other ones as they need to feed each image twice though each encoder. This results
      in twice the computation for each mini-batch. One may argue that a non-symmetric BYOL with 200 epochs should be
      compared with symmetric BYOL with 100 epochs only as they use similar amount of computation. Note that symmetric 
      MoCo v2 with 400 epochs is almost the same as asymmetric MoCo v2 with 800 epochs (71.0 vs. 71.1). * denotes that
      CompRess is not directly comparable as it trains ResNet50 using a larger SSL teacher model (SimCLR-ResNet50x4). 
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table1.png" width="100%" alt style>
    
    </p>
    
    <p>
        Additionally, we evaluate our models on transfer tasks using nearest neighbor evaluation. * denotes that CompRess is not directly
      comparable as it uses a larger SSL model as the teacher: MoCo-V2(ResNet50) for ResNet18 and SimCLR(ResNet50x4) for ResNet50.
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table2.png" width="60%" alt style>
    
    </p>
  
  
  <h5 id="evolution"><b>Evolution of teacher and student models</b></h5>
  
    <p>
       
      For every 10 epoch of ResNet-18, we evaluate both teacher and student models for BYOL, MoCo, and ISD methods using nearest neighbor. 
      For all methods, the teacher performs usually better than the student in the initial epochs when the learning rate is small and then is very close to the student when learning rate shrinks.
   
  Interestingly, the teacher performs better than the student before shrinking the learning rate. Most previous works use 
  the student as the final model which seems to be sub-ptimal. We believe this is due to ensembling effect similar to [10].
    
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/fig2.png" width="100%" alt style>
    
    </p>

    
    <h5 id="augmentation biases"><b>Removing the bias of aggressive augmentations </b></h5>
    
<p>
  Most recent self-supervised learning methods work by pulling embeddings of two different augmentations of an image close to each other. 
  It is shown that using aggressive augmentations improves the performance probably by introducing more variation between the pair of augmented images.
  Hence, using aggressive augmentation is standard in the new SSL methods. However, the augmented images do not look natural anymore, so the SSL model is 
  trained on non-natural images. We argue that this bias may reduce the quality of the features in the final tasks where the images will be natural.
  </p>
  <p>
  We use a variation of our method to remove the bias of aggressive augmentations that most recent SSL methods are trained with. 
    Our ISD method performs similarity-based distillation iteratively where the student is optimized by distilling the teacher 
    and the teacher is updated as a moving average of the student. One can initialize the teacher from an already trained model and set the momentum to 1,
    which will ignore updating the teacher. Such a method can distill the teacher into a student model that is initialized from scratch. 
    Note that this method is closely related to CompRess [1] except that we are using it for self-distillation instead of compressing from a deep model to a shallow one.
   
  We believe the student model performs better than the teacher on transfer learning settings since it has learned less unnatural biases.  
  We use Linear evaluation and NN for evaluation.
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table4.png" width="100%" alt style>
    
    </p>
  
 
<h5 id="unbalanced dataset"><b>Self-Supervised  Learning  on  Unbalanced Dataset </b></h5>
    
<p>
  To study our method on unbalanced data, we design a controlled setting to introduce the unbalanced data in the SSL training only
  and factor out its effect in the feature evaluation step. Hence, we subsample ImageNet data with 38 random categories where 8 categories are large 
  (use all almost 1300 images per category) and 30 categories are small (use only 100 images per category.) 
  We train our SSL method and then evaluate by nearest neighbor (NN) classifier on the balanced validation data. 
  To make sure that the feature evaluation is not affected by the unbalanced data, we keep both evaluation and the training data of NN search balanced, 
  so for NN search, we use all ImageNet training images (almost 1300 x 38 images) for those 38 categories. 
  We repeat the sampling of 38 categories 10 times to come up with 10 datasets.
          ``Diff'' shows the improvement of our method over MoCo. Interestingly the 
  improvement is bigger in the rare categories. This is aligned with out hypothesis that our method can handle unbalanced, unlabeled 
  data better since it does not consider all negative images equally negative.

        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table5.png" width="90%" alt style>
    
    </p>
    
    


  
<h5 id="cluster"><b>Cluster Visualizations</b></h5>   
    
    <p> We cluster ImageNet dataset into 1000 clusters using k-means and show random samples from random clusters.  
      Each row corresponds to a cluster. Note that semantically similar images are clustered together.
    </p>    
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/clusters.jpg" width="100%" alt style></p>

<h5 id="references"><b>References</b></h5>
  <br>[1] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised learning by compressing representations. Advances in Neural Information Processing Systems, 33, 2020.
  <br>[2] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-otr Bojanowski, and Armand Joulin.   Unsupervised learning of visual features by contrasting cluster assignments. arXivpreprint arXiv:2006.09882, 2020.
  <br>[3] Jean-Bastien  Grill,  Florian  Strub,  Florent  Altche,  Corentin Tallec,  Pierre  H  Richemond,  Elena  Buchatskaya,  Carl  Doersch,  Bernardo Avila Pires,  Zhaohan Daniel Guo,  Mohammad Gheshlaghi Azar,  et al.   Bootstrap your own latent:  A new  approach  to  self-supervised  learning. arXiv  preprintarXiv:2006.07733, 2020.
  <br>[4] Kaiming He,  Haoqi Fan,  Yuxin Wu,  Saining Xie,  and Ross Girshick.   Momentum  contrast  for  unsupervised  visual  representation learning.  InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738, 2020.
  <br>[5] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, ChenSun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie.  The inaturalist species classification and detectiondataset, 2018.
  <br>[6] onathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3dobject representations for fine-grained categorization. InProceedings of the IEEE international conference on computervision workshops, pages 554–561, 2013.
  <br>[7] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.Finegrained visual classification of aircraft. Technical report,2013.
  <br>[8] Maria-Elena  Nilsback  and  Andrew  Zisserman.    Automated flower  classification  over  a  large  number  of  classes.   InIndian  Conference  on  Computer  Vision,  Graphics  and  Image Processing, Dec 2008.
  <br>[9] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie,  and  P.  Perona.   Caltech-UCSD  Birds  200.   Technical  Report  CNS-TR-2010-001,  California  Institute  of  Technology, 2010.
  <br>[10] Antti Tarvainen and Harri Valpola.  Mean teachers are betterrole models:  Weight-averaged consistency targets improve semi-supervised deep learning results. InAdvances in neural information processing systems, pages 1195–1204, 2017.
  
  
